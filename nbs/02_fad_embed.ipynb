{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fad_embed\n",
    "\n",
    "> Generate embeddings from audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp fad_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import argparse\n",
    "import laion_clap \n",
    "from laion_clap.training.data import get_audio_features\n",
    "from accelerate import Accelerator\n",
    "import warnings\n",
    "import torch\n",
    "\n",
    "from aeiou.core import get_device, load_audio, get_audio_filenames, makedir\n",
    "from aeiou.datasets import AudioDataset\n",
    "from aeiou.hpc import HostPrinter\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "\n",
    "def setup_embedder(model_choice, device):\n",
    "    \"load the embedder model\"\n",
    "    embedder = None\n",
    "    if model_choice == 'clap':\n",
    "        clap_fusion, clap_amodel = True, \"HTSAT-base\"\n",
    "        #doesn't work:  warnings.filterwarnings('ignore')  # temporarily disable CLAP warnings as they are super annoying. \n",
    "        clap_module = laion_clap.CLAP_Module(enable_fusion=clap_fusion, device=device, amodel=clap_amodel).requires_grad_(False).eval()\n",
    "        clap_ckpt_path = os.environ['CLAP_CKPT']  # you'll need access to this .ckpt file\n",
    "        if clap_ckpt_path:\n",
    "            #print(f\"Loading CLAP from {clap_ckpt_path}\")\n",
    "            clap_module.load_ckpt(ckpt=clap_ckpt_path, verbose=False)\n",
    "        else:\n",
    "            clap_module.load_ckpt(model_id=1, verbose=False)\n",
    "        #warnings.filterwarnings(\"default\")   # turn warnings back on. \n",
    "        embedder = clap_module # synonyms \n",
    "    else:\n",
    "        raise ValueError(\"Sorry, other models not supported yet\")\n",
    "        \n",
    "    return embedder\n",
    "\n",
    "\n",
    "\n",
    "def embed_all(args): \n",
    "    model_choice, real_path, fake_path, chunk_size, sr, batch_size = args.embed_model, args.real_path, args.fake_path, args.chunk_size, args.sr, args.batch_size\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    ddps = f\"[{local_rank}/{world_size}]\"  # string for distributed computing info, e.g. \"[1/8]\" \n",
    "\n",
    "    accelerator = Accelerator()\n",
    "    hprint = HostPrinter(accelerator)  # hprint only prints on head node\n",
    "    device = accelerator.device  # get_device()\n",
    "    hprint(f\"{ddps} args = {args}\")\n",
    "    hprint(f'{ddps} Using device: {device}')\n",
    "    \n",
    " \n",
    "    \"\"\"\n",
    "    # get the list(s) of audio files\n",
    "    real_filenames = get_audio_filenames(real_path)\n",
    "    #hprint(f\"{ddps} real_path, real_filenames = {real_path}, {real_filenames}\")\n",
    "    fake_filenames = get_audio_filenames(fake_path)\n",
    "    minlen = len(real_filenames)\n",
    "    if len(real_filenames) != len(fake_filenames):\n",
    "        hprint(f\"{ddps} WARNING: len(real_filenames)=={len(real_filenames)} != len(fake_filenames)=={len(fake_filenames)}. Truncating to shorter list\") \n",
    "        minlen = min( len(real_filenames) , len(fake_filenames) )\n",
    "    \n",
    "    # subdivide file lists by process number\n",
    "    num_per_proc = minlen // world_size\n",
    "    start = local_rank * num_per_proc\n",
    "    end =  minlen if local_rank == world_size-1 else (local_rank+1) * num_per_proc\n",
    "    #print(f\"{ddps} start, end = \",start,end) \n",
    "    real_filenames, fake_filenames = real_filenames[start:end], fake_filenames[start:end]\n",
    "    \"\"\"\n",
    "\n",
    "    # setup embedder and dataloader\n",
    "    embedder = setup_embedder(model_choice, device)\n",
    "    hprint(f\"{ddps} Embedder '{model_choice}' ready to go!\")\n",
    "\n",
    "    real_dataset = AudioDataset(real_path, augs='Stereo(), PhaseFlipper()', sample_size=chunk_size, return_dict=True)\n",
    "    fake_dataset = AudioDataset(fake_path, augs='Stereo(), PhaseFlipper()', sample_size=chunk_size, return_dict=True)\n",
    "    real_dl = DataLoader(real_dataset, batch_size=16, shuffle=False)\n",
    "    fake_dl = DataLoader(fake_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    real_dl, fake_dl, embedder = accelerator.prepare( real_dl, fake_dl, embedder )  # prepare handles distributing things among GPUs\n",
    "    \n",
    "    # note that we don't actually care if real & fake files are pulled in the same order; we'll be comparing the *distributions* of the data.\n",
    "    for dl, name in zip([real_dl, fake_dl],['real','fake']):\n",
    "        newdir_already = False\n",
    "        for i, data_dict in enumerate(dl):\n",
    "            audio, filename_batch = data_dict['inputs'], data_dict['filename']\n",
    "            if not newdir_already: \n",
    "                p = Path( filename_batch[0] )\n",
    "                dir_already = True\n",
    "                newdir = str(p.parents[0])+\"_emb\"\n",
    "                hprint(f\"newdir = {newdir}\")\n",
    "                makedir(newdir) \n",
    "                \n",
    "            #print(f\"{ddps} i = {i}/{len(real_dataset)}, filename = {filename_batch[0]}\")\n",
    "            audio = audio.to(device)\n",
    "            while len(audio.shape) < 3: \n",
    "                audio = audio.unsqueeze(0) # add batch and/or channel dims \n",
    "                \n",
    "            embeddings = embedder.get_audio_embedding_from_data(audio.mean(dim=1).to(device), use_tensor=True).to(audio.dtype)\n",
    "            hprint(f\"embeddings.shape = {embeddings.shape}\")\n",
    "            # TODO: for now we'll just dump each batch on each proc to its own file; this could be improved\n",
    "            outfilename = f\"{newdir}/emb_p{local_rank}_b{i}.pt\"\n",
    "            print(f\"{ddps} Saving embeddings to {outfilename}\")\n",
    "            torch.save(embeddings, outfilename)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling sequence\n",
    "```\n",
    "accelerate launch fad_pytorch/fad_embed.py clap real/ fake/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def main(): \n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('embed_model', help='chioce of embedding model', default='clap')\n",
    "    parser.add_argument('real_path', help='Path of files of real audio', default='real/')\n",
    "    parser.add_argument('fake_path', help='Path of files of fake audio', default='fake/')\n",
    "    parser.add_argument('--chunk_size', type=int, default=24000, help='Length of chunks (in audio samples) to embed')\n",
    "    parser.add_argument('--batch_size', type=int, default=16, help='Batch size for computing embeddings')\n",
    "    parser.add_argument('--sr', type=int, default=48000, help='sample rate (will resample inputs at this rate)')\n",
    "    args = parser.parse_args()\n",
    "    embed_all(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if __name__ == '__main__' and \"get_ipython\" not in dir():\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa",
   "language": "python",
   "name": "aa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
