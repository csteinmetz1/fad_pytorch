{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fad_embed\n",
    "\n",
    "> Generate embeddings from audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp fad_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample calling sequence\n",
    "Multiple GPUs:\n",
    "```\n",
    "accelerate launch fad_pytorch/fad_embed.py clap real/ fake/\n",
    "```\n",
    "\n",
    "General invocation: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import argparse\n",
    "import laion_clap \n",
    "from laion_clap.training.data import get_audio_features\n",
    "from accelerate import Accelerator\n",
    "import warnings\n",
    "import torch\n",
    "\n",
    "from aeiou.core import get_device, load_audio, get_audio_filenames, makedir\n",
    "from aeiou.datasets import AudioDataset\n",
    "from aeiou.hpc import HostPrinter\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import requests \n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    from fad_pytorch.pann import Cnn14_16k\n",
    "except: \n",
    "    from pann import Cnn14_16k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a couple utilities for downloading checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "\n",
    "def download_file(url, local_filename):\n",
    "    \"Includes a progress bar.  from https://stackoverflow.com/a/37573701/4259243\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size_in_bytes= int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024 #1 Kilobye\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "    with open(local_filename, 'wb') as file:\n",
    "        for data in response.iter_content(block_size):\n",
    "            progress_bar.update(len(data))\n",
    "            file.write(data)\n",
    "    progress_bar.close()\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print(\"ERROR, something went wrong\")\n",
    "    return local_filename\n",
    "\n",
    "def get_ckpt(ckpt_file='music_speech_audioset_epoch_15_esc_89.98.pt',\n",
    "             ckpt_base_url='https://huggingface.co/lukewys/laion_clap/blob/main',\n",
    "             ckpt_dl_path=os.path.expanduser(\"~/checkpoints\")\n",
    "            ):\n",
    "    ckpt_path = f\"{ckpt_dl_path}/{ckpt_file}\"\n",
    "    if not os.path.isfile(ckpt_path):\n",
    "        print(f\"File {ckpt_path} not found, downloading from {ckpt_base_url}/{ckpt_file}\")\n",
    "        download_file( f\"{ckpt_base_url}/{ckpt_file}\", ckpt_path)\n",
    "    return ckpt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "def setup_embedder(\n",
    "        model_choice='clap', # 'clap' | 'vggish' | 'pann'\n",
    "        device='cuda',\n",
    "        ckpt_file='music_speech_audioset_epoch_15_esc_89.98.pt',  # NOTE: 'CLAP_CKPT' env var overrides ckpt_file kwarg\n",
    "        ckpt_base_url='https://huggingface.co/lukewys/laion_clap/resolve/main',\n",
    "        # https://huggingface.co/lukewys/laion_clap/resolve/main/music_speech_audioset_epoch_15_esc_89.98.pt\n",
    "        accelerator=None,\n",
    "        ckpt_dl_path=os.path.expanduser(\"~/checkpoints\"),\n",
    "    ):\n",
    "    \"load the embedder model\"\n",
    "    embedder = None\n",
    "    \n",
    "    if model_choice == 'clap':\n",
    "        clap_fusion, clap_amodel = True, \"HTSAT-base\"\n",
    "        #doesn't work:  warnings.filterwarnings('ignore')  # temporarily disable CLAP warnings as they are super annoying. \n",
    "        clap_module = laion_clap.CLAP_Module(enable_fusion=clap_fusion, device=device, amodel=clap_amodel).requires_grad_(False).eval()\n",
    "        clap_ckpt_path = os.getenv('CLAP_CKPT')  # NOTE: CLAP_CKPT env var overrides ckpt_file kwarg\n",
    "        if clap_ckpt_path is not None:\n",
    "            #print(f\"Loading CLAP from {clap_ckpt_path}\")\n",
    "            clap_module.load_ckpt(ckpt=clap_ckpt_path, verbose=False)\n",
    "        else:\n",
    "            if accelerator is None or accelerator.is_main_process: print(f\"No CLAP checkpoint specified, using {ckpt_file}\") \n",
    "            clap_module = laion_clap.CLAP_Module(enable_fusion=False, amodel= 'HTSAT-base')\n",
    "            ckpt_path = get_ckpt(ckpt_file=ckpt_file, ckpt_base_url=ckpt_base_url, ckpt_dl_path=ckpt_dl_path)\n",
    "            clap_module.load_ckpt(ckpt_path, verbose=False)\n",
    "            #clap_module.load_ckpt(model_id=1, verbose=False)\n",
    "        #warnings.filterwarnings(\"default\")   # turn warnings back on. \n",
    "        embedder = clap_module # synonyms \n",
    "        sample_rate = 48000\n",
    "        \n",
    "    # next two model loading codes from gudgud96's repo: https://github.com/gudgud96/frechet-audio-distance, LICENSE below\n",
    "    elif model_choice == \"vggish\":   # https://arxiv.org/abs/1609.09430\n",
    "        embedder = torch.hub.load('harritaylor/torchvggish', 'vggish')\n",
    "        use_pca=False\n",
    "        use_activation=False\n",
    "        if not use_pca:  embedder.postprocess = False\n",
    "        if not use_activation: embedder.embeddings = torch.nn.Sequential(*list(embedder.embeddings.children())[:-1])\n",
    "        sample_rate = 16000\n",
    "\n",
    "    elif model_choice == \"pann\": # https://arxiv.org/abs/1912.10211\n",
    "        model_path = os.path.join(torch.hub.get_dir(), \"Cnn14_16k_mAP%3D0.438.pth\")\n",
    "        if not(os.path.exists(model_path)):\n",
    "            torch.hub.download_url_to_file('https://zenodo.org/record/3987831/files/Cnn14_16k_mAP%3D0.438.pth', model_path)\n",
    "        embedder = Cnn14_16k(sample_rate=16000, window_size=512, hop_size=160, mel_bins=64, fmin=50, fmax=8000, classes_num=527)\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        embedder.load_state_dict(checkpoint['model'])\n",
    "        sample_rate = 16000\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Sorry, other models not supported yet\")\n",
    "        \n",
    "    embedder.eval()   \n",
    "    return embedder, sample_rate\n",
    "\n",
    "\n",
    "GUDGUD_LICENSE = \"\"\"\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2022 Hao Hao Tan\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CLAP checkpoint specified, using music_speech_audioset_epoch_15_esc_89.98.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /home/shawley/checkpoints/music_speech_audioset_epoch_15_esc_89.98.pt not found, downloading from https://huggingface.co/lukewys/laion_clap/resolve/main/music_speech_audioset_epoch_15_esc_89.98.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.35G/2.35G [00:16<00:00, 142MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the specified checkpoint /home/shawley/checkpoints/music_speech_audioset_epoch_15_esc_89.98.pt from users.\n",
      "Load Checkpoint...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(CLAP_Module(\n",
       "   (model): CLAP(\n",
       "     (audio_branch): HTSAT_Swin_Transformer(\n",
       "       (spectrogram_extractor): Spectrogram(\n",
       "         (stft): STFT(\n",
       "           (conv_real): Conv1d(1, 513, kernel_size=(1024,), stride=(480,), bias=False)\n",
       "           (conv_imag): Conv1d(1, 513, kernel_size=(1024,), stride=(480,), bias=False)\n",
       "         )\n",
       "       )\n",
       "       (logmel_extractor): LogmelFilterBank()\n",
       "       (spec_augmenter): SpecAugmentation(\n",
       "         (time_dropper): DropStripes()\n",
       "         (freq_dropper): DropStripes()\n",
       "       )\n",
       "       (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (patch_embed): PatchEmbed(\n",
       "         (proj): Conv2d(1, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "         (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "       (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "       (layers): ModuleList(\n",
       "         (0): BasicLayer(\n",
       "           dim=128, input_resolution=(64, 64), depth=2\n",
       "           (blocks): ModuleList(\n",
       "             (0): SwinTransformerBlock(\n",
       "               dim=128, input_resolution=(64, 64), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "               (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 dim=128, window_size=(8, 8), num_heads=4\n",
       "                 (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): Identity()\n",
       "               (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                 (act): GELU(approximate='none')\n",
       "                 (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (1): SwinTransformerBlock(\n",
       "               dim=128, input_resolution=(64, 64), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "               (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 dim=128, window_size=(8, 8), num_heads=4\n",
       "                 (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                 (act): GELU(approximate='none')\n",
       "                 (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (downsample): PatchMerging(\n",
       "             input_resolution=(64, 64), dim=128\n",
       "             (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "             (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "           )\n",
       "         )\n",
       "         (1): BasicLayer(\n",
       "           dim=256, input_resolution=(32, 32), depth=2\n",
       "           (blocks): ModuleList(\n",
       "             (0): SwinTransformerBlock(\n",
       "               dim=256, input_resolution=(32, 32), num_heads=8, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "               (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 dim=256, window_size=(8, 8), num_heads=8\n",
       "                 (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                 (act): GELU(approximate='none')\n",
       "                 (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (1): SwinTransformerBlock(\n",
       "               dim=256, input_resolution=(32, 32), num_heads=8, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "               (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 dim=256, window_size=(8, 8), num_heads=8\n",
       "                 (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                 (act): GELU(approximate='none')\n",
       "                 (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (downsample): PatchMerging(\n",
       "             input_resolution=(32, 32), dim=256\n",
       "             (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "             (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           )\n",
       "         )\n",
       "         (2): BasicLayer(\n",
       "           dim=512, input_resolution=(16, 16), depth=12\n",
       "           (blocks): ModuleList(\n",
       "             (0): SwinTransformerBlock(\n",
       "               dim=512, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "               (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 dim=512, window_size=(8, 8), num_heads=16\n",
       "                 (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                 (act): GELU(approximate='none')\n",
       "                 (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (1): SwinTransformerBlock(\n",
       "               dim=512, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "               (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 dim=512, window_size=(8, 8), num_heads=16\n",
       "                 (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                 (act): GELU(approximate='none')\n",
       "                 (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (2): SwinTransformerBlock(\n",
       "               dim=512, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "               (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 dim=512, window_size=(8, 8), num_heads=16\n",
       "                 (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                 (act): GELU(approximate='none')\n",
       "                 (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (3): SwinTransformerBlock(\n",
       "               dim=512, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "               (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 dim=512, window_size=(8, 8), num_heads=16\n",
       "                 (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                 (act): GELU(approximate='none')\n",
       "                 (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (4): SwinTransformerBlock(\n",
       "               dim=512, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "               (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 dim=512, window_size=(8, 8), num_heads=16\n",
       "                 (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                 (act): GELU(approximate='none')\n",
       "                 (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (5): SwinTransformerBlock(\n",
       "               dim=512, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "               (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 dim=512, window_size=(8, 8), num_heads=16\n",
       "                 (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                 (act): GELU(approximate='none')\n",
       "                 (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (6): SwinTransformerBlock(\n",
       "               dim=512, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "               (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 dim=512, window_size=(8, 8), num_heads=16\n",
       "                 (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                 (act): GELU(approximate='none')\n",
       "                 (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (7): SwinTransformerBlock(\n",
       "               dim=512, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "               (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 dim=512, window_size=(8, 8), num_heads=16\n",
       "                 (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                 (act): GELU(approximate='none')\n",
       "                 (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (8): SwinTransformerBlock(\n",
       "               dim=512, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "               (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 dim=512, window_size=(8, 8), num_heads=16\n",
       "                 (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                 (act): GELU(approximate='none')\n",
       "                 (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (9): SwinTransformerBlock(\n",
       "               dim=512, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "               (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 dim=512, window_size=(8, 8), num_heads=16\n",
       "                 (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                 (act): GELU(approximate='none')\n",
       "                 (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (10): SwinTransformerBlock(\n",
       "               dim=512, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "               (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 dim=512, window_size=(8, 8), num_heads=16\n",
       "                 (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                 (act): GELU(approximate='none')\n",
       "                 (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (11): SwinTransformerBlock(\n",
       "               dim=512, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "               (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 dim=512, window_size=(8, 8), num_heads=16\n",
       "                 (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                 (act): GELU(approximate='none')\n",
       "                 (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (downsample): PatchMerging(\n",
       "             input_resolution=(16, 16), dim=512\n",
       "             (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "             (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "           )\n",
       "         )\n",
       "         (3): BasicLayer(\n",
       "           dim=1024, input_resolution=(8, 8), depth=2\n",
       "           (blocks): ModuleList(\n",
       "             (0-1): 2 x SwinTransformerBlock(\n",
       "               dim=1024, input_resolution=(8, 8), num_heads=32, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "               (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "               (attn): WindowAttention(\n",
       "                 dim=1024, window_size=(8, 8), num_heads=32\n",
       "                 (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                 (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                 (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                 (softmax): Softmax(dim=-1)\n",
       "               )\n",
       "               (drop_path): DropPath()\n",
       "               (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "               (mlp): Mlp(\n",
       "                 (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                 (act): GELU(approximate='none')\n",
       "                 (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "       (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "       (maxpool): AdaptiveMaxPool1d(output_size=1)\n",
       "       (tscam_conv): Conv2d(1024, 527, kernel_size=(2, 3), stride=(1, 1), padding=(0, 1))\n",
       "       (head): Linear(in_features=527, out_features=527, bias=True)\n",
       "     )\n",
       "     (text_branch): RobertaModel(\n",
       "       (embeddings): RobertaEmbeddings(\n",
       "         (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "         (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "         (token_type_embeddings): Embedding(1, 768)\n",
       "         (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (encoder): RobertaEncoder(\n",
       "         (layer): ModuleList(\n",
       "           (0-11): 12 x RobertaLayer(\n",
       "             (attention): RobertaAttention(\n",
       "               (self): RobertaSelfAttention(\n",
       "                 (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "               (output): RobertaSelfOutput(\n",
       "                 (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                 (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (intermediate): RobertaIntermediate(\n",
       "               (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "               (intermediate_act_fn): GELUActivation()\n",
       "             )\n",
       "             (output): RobertaOutput(\n",
       "               (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (pooler): RobertaPooler(\n",
       "         (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "         (activation): Tanh()\n",
       "       )\n",
       "     )\n",
       "     (text_transform): MLPLayers(\n",
       "       (nonlin): ReLU()\n",
       "       (sequential): Sequential(\n",
       "         (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "         (1): ReLU()\n",
       "         (2): Dropout(p=0.1, inplace=False)\n",
       "         (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "       )\n",
       "     )\n",
       "     (text_projection): Sequential(\n",
       "       (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "       (1): ReLU()\n",
       "       (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "     )\n",
       "     (audio_transform): MLPLayers(\n",
       "       (nonlin): ReLU()\n",
       "       (sequential): Sequential(\n",
       "         (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "         (1): ReLU()\n",
       "         (2): Dropout(p=0.1, inplace=False)\n",
       "         (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "       )\n",
       "     )\n",
       "     (audio_projection): Sequential(\n",
       "       (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "       (1): ReLU()\n",
       "       (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 48000)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "setup_embedder('clap','cuda') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def embed_all(args): \n",
    "    model_choice, real_path, fake_path, chunk_size, sr, max_batch_size = args.embed_model, args.real_path, args.fake_path, args.chunk_size, args.sr, args.batch_size\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    ddps = f\"[{local_rank}/{world_size}]\"  # string for distributed computing info, e.g. \"[1/8]\" \n",
    "\n",
    "    accelerator = Accelerator()\n",
    "    hprint = HostPrinter(accelerator)  # hprint only prints on head node\n",
    "    device = accelerator.device    # get_device()\n",
    "    hprint(f\"{ddps} args = {args}\")\n",
    "    hprint(f'{ddps} Using device: {device}')\n",
    "    \n",
    " \n",
    "    \"\"\" # let accelerate split up the files among processsors\n",
    "    # get the list(s) of audio files\n",
    "    real_filenames = get_audio_filenames(real_path)\n",
    "    #hprint(f\"{ddps} real_path, real_filenames = {real_path}, {real_filenames}\")\n",
    "    fake_filenames = get_audio_filenames(fake_path)\n",
    "    minlen = len(real_filenames)\n",
    "    if len(real_filenames) != len(fake_filenames):\n",
    "        hprint(f\"{ddps} WARNING: len(real_filenames)=={len(real_filenames)} != len(fake_filenames)=={len(fake_filenames)}. Truncating to shorter list\") \n",
    "        minlen = min( len(real_filenames) , len(fake_filenames) )\n",
    "    \n",
    "    # subdivide file lists by process number\n",
    "    num_per_proc = minlen // world_size\n",
    "    start = local_rank * num_per_proc\n",
    "    end =  minlen if local_rank == world_size-1 else (local_rank+1) * num_per_proc\n",
    "    #print(f\"{ddps} start, end = \",start,end) \n",
    "    real_filenames, fake_filenames = real_filenames[start:end], fake_filenames[start:end]\n",
    "    \"\"\"\n",
    "\n",
    "    # setup embedder and dataloader\n",
    "    embedder, emb_sample_rate = setup_embedder(model_choice, device, accelerator)\n",
    "    if sr != emb_sample_rate:\n",
    "        hprint(f\"\\n*******\\nWARNING: sr={sr} != {model_choice}'s emb_sample_rate={emb_sample_rate}. Will resample audio to the latter\\n*******\\n\")\n",
    "        sr = emb_sample_rate\n",
    "    hprint(f\"{ddps} Embedder '{model_choice}' ready to go!\")\n",
    "\n",
    "    real_dataset = AudioDataset(real_path, augs='Stereo(), PhaseFlipper()', sample_rate=emb_sample_rate, sample_size=chunk_size, return_dict=True, verbose=args.verbose)\n",
    "    fake_dataset = AudioDataset(fake_path, augs='Stereo(), PhaseFlipper()', sample_rate=emb_sample_rate, sample_size=chunk_size, return_dict=True, verbose=args.verbose)\n",
    "    batch_size = min( len(real_dataset) // world_size , max_batch_size ) \n",
    "    hprint(f\"\\nGiven max_batch_size = {max_batch_size}, len(real_dataset) = {len(real_dataset)}, and world_size = {world_size}, we'll use batch_size = {batch_size}\")\n",
    "    real_dl = DataLoader(real_dataset, batch_size=batch_size, shuffle=False)\n",
    "    fake_dl = DataLoader(fake_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    real_dl, fake_dl, embedder = accelerator.prepare( real_dl, fake_dl, embedder )  # prepare handles distributing things among GPUs\n",
    "    \n",
    "    # note that we don't actually care if real & fake files are pulled in the same order; we'll only be comparing the *distributions* of the data.\n",
    "    with torch.no_grad():\n",
    "        for dl, name in zip([real_dl, fake_dl],['real','fake']):\n",
    "            newdir_already = False\n",
    "            for i, data_dict in enumerate(dl):\n",
    "                audio, filename_batch = data_dict['inputs'], data_dict['filename']\n",
    "                if not newdir_already: \n",
    "                    p = Path( filename_batch[0] )\n",
    "                    dir_already = True\n",
    "                    newdir = f\"{p.parents[0]}_emb_{model_choice}\"\n",
    "                    hprint(f\"newdir = {newdir}\")\n",
    "                    makedir(newdir) \n",
    "\n",
    "                #print(f\"{ddps} i = {i}/{len(real_dataset)}, filename = {filename_batch[0]}\")\n",
    "                audio = audio.to(device)\n",
    "\n",
    "\n",
    "                if model_choice == 'clap': \n",
    "                    while len(audio.shape) < 3: \n",
    "                        audio = audio.unsqueeze(0) # add batch and/or channel dims \n",
    "                    embeddings = embedder.get_audio_embedding_from_data(audio.mean(dim=1).to(device), use_tensor=True).to(audio.dtype)\n",
    "\n",
    "                elif model_choice == \"vggish\":\n",
    "                    audio = torch.mean(audio, dim=1)   # vggish requries we convert to mono\n",
    "                    embeddings = []                    # ...whoa, vggish can't even handle batches?  we have to pass 'em through singly?\n",
    "                    for bi, waveform in enumerate(audio): \n",
    "                        e = embedder.forward(waveform.cpu().numpy(), emb_sample_rate)\n",
    "                        embeddings.append(e) \n",
    "                    embeddings = torch.cat(embeddings, dim=0)\n",
    "\n",
    "                elif model_choice == \"pann\": \n",
    "                    audio = torch.mean(audio, dim=1)  # mono only.  todo:  keepdim=True ?\n",
    "                    out = embedder.forward(audio, None)\n",
    "                    embeddings = out['embedding'].data\n",
    "\n",
    "                hprint(f\"embeddings.shape = {embeddings.shape}\")\n",
    "                # TODO: for now we'll just dump each batch on each proc to its own file; this could be improved\n",
    "                outfilename = f\"{newdir}/emb_p{local_rank}_b{i}.pt\"\n",
    "                print(f\"{ddps} Saving embeddings to {outfilename}\")\n",
    "                torch.save(embeddings.cpu().detach(), outfilename)\n",
    "    return        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def main(): \n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('embed_model', help='choice of embedding model: clap | vggish | pann', default='clap')\n",
    "    parser.add_argument('real_path', help='Path of files of real audio', default='real/')\n",
    "    parser.add_argument('fake_path', help='Path of files of fake audio', default='fake/')\n",
    "    parser.add_argument('--chunk_size', type=int, default=24000, help='Length of chunks (in audio samples) to embed')\n",
    "    parser.add_argument('--batch_size', type=int, default=64, help='MAXIMUM Batch size for computing embeddings (may go smaller)')\n",
    "    parser.add_argument('--sr', type=int, default=48000, help='sample rate (will resample inputs at this rate)')\n",
    "    parser.add_argument('--verbose', action='store_true',  help='Show notices of resampling when reading files')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    embed_all(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if __name__ == '__main__' and \"get_ipython\" not in dir():\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa",
   "language": "python",
   "name": "aa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
