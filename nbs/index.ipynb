{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#from fad_pytorch.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fad_pytorch\n",
    "\n",
    "> Frechet Audio Distance evaluation in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Original FAD paper (PDF)](https://arxiv.org/pdf/1812.08466.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install fad_pytorch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features:\n",
    "\n",
    "- runs in parallel on multiple processors and multiple GPUs (via `accelerate`)\n",
    "- supports multiple embedding methods:\n",
    "     -  VGGish and PANN, both mono @ 16kHz\n",
    "     -  OpenL3 and (LAION-)CLAP, stereo @ 48kHz\n",
    "- favors ops in PyTorch rather than numpy (or tensorflow)\n",
    "- `fad_gen` supports WebDataset (audio data stored in S3 buckets)\n",
    "- runs on CPU, CUDA, or MPS \n",
    "\n",
    "## Instructions:\n",
    "\n",
    "This is designed to be run as 3 command-line scripts in succession. The latter 2 (`fad_embed` and `fad_score`) are probably what most people will want:\n",
    "\n",
    "1. `fad_gen`: produces directories of real & fake audio. See `fad_gen` [documentation](https://drscotthawley.github.io/fad_pytorch/fad_gen.html) for calling sequence.\n",
    "2. `fad_embed [options] <real_audio_dir> <fake_audio_dir>`: produces directories of *embeddings* of real & fake audio\n",
    "3. `fad_score [options] <real_emb_dir> <fake_emb_dir>`: reads the embeddings & generates FAD score, for real (\"$r$\") and fake (\"$f$\"): \n",
    "\n",
    "$$ FAD = || \\mu_r - \\mu_f ||^2 + tr\\left(\\Sigma_r + \\Sigma_f - 2 \\sqrt{\\Sigma_r \\Sigma_f}\\right)$$\n",
    "\n",
    "## Documentation\n",
    "See the [Documentation Website](https://drscotthawley.github.io/fad_pytorch/) for more. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments / FAQ / Troubleshooting\n",
    "\n",
    "- \"`RuntimeError: CUDA error: invalid device ordinal`\": This happens when you have a \"bad node\" on an AWS cluster. [Haven't yet figured out what causes it or how to fix it](https://discuss.huggingface.co/t/solved-accelerate-accelerator-cuda-error-invalid-device-ordinal/21509/1).  Workaround: Just add the current node to your SLURM `--exclude` list, exit and retry.  Note: it may take as many as 5 to 7 retries before you get a \"good node\". \n",
    "- \"FAD scores obtained from different embedding methods are *wildly* different!\" ...Yea. It's not obvious that scores from different embedding methods should be comparable.  Rather, compare different groups of audio files using the same embedding method, and/or check that FAD scores go *down* as similarity improves.\n",
    "- \"FAD score for the same dataset repeated (twice) is not exactly zero!\"  ...Yea. There seems to be an uncertainty of around +/- 0.008.  I'd say, don't quote any numbers past the first decimal point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributing\n",
    "This repo is still fairly \"bare bones\" and will benefit from more documentation and features as time goes on.  Note that it is written using [nbdev](https://nbdev.fast.ai/), so the things to do oare:\n",
    "\n",
    "1. Fork this repo\n",
    "1. Clone your fork to your (local) machine \n",
    "1. Install nbdev: `python3 -m pip install -U nbdev`\n",
    "1. Make changes by editing the notebooks in `nbs/`, not the `.py` files in `fad_pytorch/`. \n",
    "1. Run `nbdev_export` to export notebook changes to `.py` files \n",
    "1. For good measure, run `nbdev_install_hooks` and `nbdev_clean` - especially if you've *added* any notebooks. \n",
    "1. Do a `git status` to see all the `.ipynb` and `.py` files that need to be added & committed\n",
    "1. `git add` those files and then `git commit`, and then `git push`\n",
    "1. Take a look in your fork's GitHub Actions tab, and see if the \"test\" and \"deploy\" CI runs finish properly (green light) or fail (red light) \n",
    "1. Once you get green lights, send in a Pull Request! \n",
    "\n",
    "*Feel free to ask me for tips with nbdev, it has quite a learning curve.  You can also ask on [fast.ai forums](https://forums.fast.ai/) and/or [fast.ai Discord](https://discord.com/channels/689892369998676007/887694559952400424)* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Repos\n",
    "There are [several] others, but this one is mine.  These repos didn't have all the features I wanted, but I used them for inspiration:\n",
    "\n",
    "- https://github.com/gudgud96/frechet-audio-distance\n",
    "- https://github.com/google-research/google-research/tree/master/frechet_audio_distance: Goes with [Original FAD paper](https://arxiv.org/pdf/1812.08466.pdf)\n",
    "- https://github.com/AndreevP/speech_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa",
   "language": "python",
   "name": "aa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
