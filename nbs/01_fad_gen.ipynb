{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fad_gen\n",
    "\n",
    "> Produce directories of real and fake audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program may not be needed if you already have directories of real & fake audio.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp fad_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample calling sequence(s):\n",
    "\n",
    "Single GPU, local data:\n",
    "```\n",
    "fad_gen test autoencoder.ts \"real1/ real2/ real3/\"\n",
    "```\n",
    "\n",
    "\n",
    "Multiple GPUs, data on S3:\n",
    "```\n",
    "accelerate launch fad_pytorch/fad_gen.py 5s_simple model_checkpoint.ts \"s3://s-laion-audio/webdataset_tar/freesound_no_overlap/ s3://s-laion-audio/webdataset_tar/epidemic_sound_effects/\" -p \"{'s3://s-laion-audio':'default'}\"\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "General calling sequence:\n",
    "```\n",
    "$ fad_gen -h\n",
    "usage: fad_gen [-h] [-b BATCH_SIZE] [--n N] [--num_workers NUM_WORKERS] [-p PROFILES] [--sample_rate SAMPLE_RATE] [-s SAMPLE_SIZE]\n",
    "               name model_ckpt data_sources\n",
    "\n",
    "positional arguments:\n",
    "  name                  Name prefix for output directories: <name>_reals/ & <name>_fakes/\n",
    "  model_ckpt            TorchScript (.ts) (Generative) Model checkpoint file\n",
    "  data_sources          Space-separated string listing either S3 resources or local directories (but not a mix of both!) for real data\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -b BATCH_SIZE, --batch_size BATCH_SIZE\n",
    "                        batch size (default: 2)\n",
    "  --n N                 Number of real/fake samples to grab/generate, respectively (default: 256)\n",
    "  --num_workers NUM_WORKERS\n",
    "                        Number of pytorch workers to use in DataLoader (default: 12)\n",
    "  -p PROFILES, --profiles PROFILES\n",
    "                        String representation of dict {resource:profile} (default: )\n",
    "  --sample_rate SAMPLE_RATE\n",
    "                        sample rate (will resample inputs at this rate) (default: 48000)\n",
    "  -s SAMPLE_SIZE, --sample_size SAMPLE_SIZE\n",
    "                        Number of samples per clip (default: 262144)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import argparse\n",
    "from accelerate import Accelerator\n",
    "import warnings\n",
    "import torch\n",
    "\n",
    "from aeiou.core import get_device, load_audio, get_audio_filenames, makedir\n",
    "from aeiou.datasets import get_wds_loader, AudioDataset\n",
    "from aeiou.hpc import HostPrinter\n",
    "from pathlib import Path\n",
    "#from audio_algebra.given_models import StackedDiffAEWrapper\n",
    "import ast\n",
    "import torchaudio\n",
    "from tqdm.auto import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def gen(args):\n",
    "    \n",
    "    # HPC / parallel setup\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    ddps = f\"[{local_rank}/{world_size}]\"  # string for distributed computing info, e.g. \"[1/8]\" \n",
    "    accelerator = Accelerator()\n",
    "    hprint = HostPrinter(accelerator)  # hprint only prints on head node\n",
    "    device = accelerator.device  # get_device()\n",
    "    hprint(f\"gen: args = {args}\")\n",
    "    hprint(f'{ddps} Using device: {device}')\n",
    "    \n",
    "    \n",
    "    model_ckpt, data_sources, profiles, n = args.model_ckpt, args.data_sources, args.profiles,  args.n\n",
    "    names = data_sources.split(' ')\n",
    "    #hprint(f\"names = {names}\")\n",
    "    local_data = False\n",
    "    if 's3://' in data_sources: \n",
    "        hprint(\"Data sources are on S3\")\n",
    "        profiles = ast.literal_eval(profiles)\n",
    "        hprint(f\"profiles = {profiles}\")\n",
    "\n",
    "        dl = get_wds_loader(\n",
    "            batch_size=args.batch_size,\n",
    "            s3_url_prefix=None,\n",
    "            sample_size=args.sample_size,\n",
    "            names=names,\n",
    "            sample_rate=args.sample_rate,\n",
    "            num_workers=args.num_workers,\n",
    "            recursive=True,\n",
    "            random_crop=True,\n",
    "            epoch_steps=10000,\n",
    "            profiles=profiles,\n",
    "        )\n",
    "    else:\n",
    "        hprint(\"Data sources are local\")\n",
    "        dataset = AudioDataset(names, sample_rate=args.sample_rate, sample_size=args.sample_size)\n",
    "        dl = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, num_workers=args.num_workers)\n",
    "        local_data = True\n",
    "        \n",
    "    print(f\"loading {model_ckpt}....\")\n",
    "    if model_ckpt.endswith('.ts'):\n",
    "        model = torch.jit.load(model_ckpt)\n",
    "    #else:  # default is stacked diffae\n",
    "    #    model = StackedDiffAEWrapper(ckpt_info={'ckpt_path':model_ckpt})\n",
    "    try:\n",
    "        model.setup()  # if it needs setup call\n",
    "    except: \n",
    "        pass \n",
    "    \n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model, dl = accelerator.prepare( model, dl )  # prepare handles distributing things among GPUs\n",
    "\n",
    "    reals_path, fakes_path = f\"{args.name}_reals\", f\"{args.name}_fakes\"\n",
    "    makedir(reals_path)\n",
    "    makedir(fakes_path)\n",
    "\n",
    "    progress_bar = tqdm(range(math.ceil(args.n/args.batch_size)), disable=not accelerator.is_local_main_process)\n",
    "\n",
    "    for i, data in enumerate(dl):\n",
    "        reals = data if local_data else data[0][0]\n",
    "        if args.debug: hprint(f\"{ddps} i = {i}, reals.shape = {reals.shape}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            fakes = accelerator.unwrap_model(model).forward(reals.to(device)).cpu()\n",
    "        #hprint(f\"fakes.shape = {fakes.shape}\")\n",
    "        \n",
    "        for b in range(reals.shape[0]):\n",
    "            waveform = reals[b]\n",
    "            torchaudio.save(f\"{reals_path}/{i}_{b}.wav\", waveform.cpu(), args.sample_rate)\n",
    "            waveform = fakes[b]\n",
    "            torchaudio.save(f\"{fakes_path}/{i}_{b}.wav\", waveform.cpu(), args.sample_rate)\n",
    "            \n",
    "        progress_bar.update(1)\n",
    "        if (i+1)*args.batch_size > args.n:\n",
    "            hprint(f\"\\nGot all the data we needed: {i*args.batch_size}. Stopping\")\n",
    "            break\n",
    "    \n",
    "    hprint(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def main(): \n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('name', help='Name prefix for output directories: <name>_reals/ & <name>_fakes/')\n",
    "    parser.add_argument('model_ckpt', help='TorchScript (.ts) (Generative) Model checkpoint file')\n",
    "    parser.add_argument('data_sources', help='Space-separated string listing either S3 resources or local directories (but not a mix of both!) for real data')\n",
    "    parser.add_argument('-d','--debug', action=\"store_true\", help='Enable extra debugging messages')\n",
    "    parser.add_argument('-b',\"--batch_size\", default=2, help='batch size')\n",
    "    parser.add_argument('--n', type=int, default=256, help='Number of real/fake samples to grab/generate, respectively')\n",
    "    parser.add_argument('--num_workers', type=int, default=12, help='Number of pytorch workers to use in DataLoader')\n",
    "    parser.add_argument('-p',\"--profiles\", default='', help='String representation of dict {resource:profile} of AWS credentials')\n",
    "    parser.add_argument('--sample_rate', type=int, default=48000, help='sample rate (will resample inputs at this rate)')\n",
    "    parser.add_argument('-s','--sample_size', type=int, default=2**18, help='Number of samples per clip')\n",
    "    args = parser.parse_args()\n",
    "    gen( args )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if __name__ == '__main__' and \"get_ipython\" not in dir():\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa",
   "language": "python",
   "name": "aa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
